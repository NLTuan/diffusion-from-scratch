{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54c89c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589b57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val): return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper modules\n",
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "    \n",
    "def Downsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n",
    "    )\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_sqrt = torch.sqrt(x.pow(2).mean(dim=1, keepdim=True) + self.eps)\n",
    "        x = x/mean_sqrt * self.gamma[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84412032",
   "metadata": {},
   "source": [
    "Positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7da59200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 1, 23, 23])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(23, 3, 23, 23).mean(dim=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "        \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim = -1)\n",
    "        return emb\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d769f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = RMSNorm()\n",
    "        self.act = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "        \n",
    "        return self.dropout(self.act(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU,\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "        \n",
    "        self.block1 = Block(dim, dim_out, dropout=dropout)\n",
    "        self.block2 = Block(dim_out, dim_out)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "            \n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        out = self.block2(x)\n",
    "        return out + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cee55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4440,  0.3920,  0.6530, -0.9605,  0.2679, -0.5658, -0.9110,  1.4983,\n",
       "           0.0041,  0.8113,  2.2044,  1.1604, -0.7538, -2.2985, -0.3477,  1.7214,\n",
       "           0.5118,  0.4109, -0.2641,  1.2216, -1.1585,  0.2219,  1.3619,  0.2927,\n",
       "          -0.5375,  0.7451, -0.0975,  0.8557, -1.5112,  1.5189,  0.0909, -1.8848,\n",
       "           0.6316,  0.3313,  0.7537,  0.3601,  0.9153,  0.0710, -0.1831,  0.8386,\n",
       "          -0.4594,  0.5228, -0.8754,  0.7014,  1.9111,  1.7775, -0.3256, -0.4987,\n",
       "          -0.1167, -0.7170,  0.4379, -0.2896, -0.5781,  0.5635, -0.0133,  1.1437,\n",
       "           0.9590,  1.1001,  1.2521, -1.4323,  0.0745, -0.2219,  0.6893,  0.3561,\n",
       "          -0.3258,  0.5482, -0.3023, -0.0561, -1.3465, -1.7644, -0.0964, -0.3883,\n",
       "           0.4494, -0.2762, -0.4611, -0.9451,  1.1704, -0.5131,  0.5018, -0.3176,\n",
       "          -1.2073,  0.6973,  0.1539, -0.0280, -0.2244, -0.3203,  0.0479, -1.2624,\n",
       "          -0.2860,  0.2154, -1.1691, -2.3402, -0.1102,  0.1076, -0.1814,  1.1576,\n",
       "          -1.0266, -0.8778,  0.1122,  0.5178,  1.1391,  1.1263,  0.1709, -0.8416,\n",
       "           0.5471, -0.8313,  1.8462,  0.9255,  0.3694, -0.0935,  0.2177, -0.3144,\n",
       "          -0.6501, -0.4446, -1.0333,  0.8225,  0.6282, -0.8594, -2.6797, -0.4633,\n",
       "           0.5223,  1.2600, -1.0649,  0.4915, -1.5480,  1.4569, -1.5540, -1.3400]]),\n",
       " tensor([[-1.5802e-01, -1.9825e+00, -8.4192e-01, -3.9333e-01, -1.3157e+00,\n",
       "          -3.1823e-01, -2.8722e-01,  1.8001e-01,  5.5984e-02, -1.4215e+00,\n",
       "          -9.5038e-01,  2.8901e-01,  4.5565e-01,  3.1177e-01, -5.8460e-03,\n",
       "          -4.2000e-01, -6.3425e-02,  4.1771e-01, -1.0647e+00, -2.0939e-01,\n",
       "           6.7934e-02, -1.0355e+00, -8.4118e-01, -2.0084e-03, -9.0922e-02,\n",
       "           9.2906e-01,  8.0734e-01,  1.7031e+00,  6.0159e-01,  1.1821e+00,\n",
       "          -3.6594e-01, -2.9233e-01,  2.7903e-01,  1.1315e+00, -1.6137e+00,\n",
       "          -2.0835e+00, -3.8661e-01, -2.1818e+00,  1.6338e+00,  4.4133e-01,\n",
       "           3.1904e-01,  5.2985e-01, -9.7704e-01, -9.6988e-02, -4.2743e-01,\n",
       "           1.8611e+00,  3.0621e-01,  3.5608e-01, -4.5973e-01, -2.0759e+00,\n",
       "           1.1349e+00, -1.1273e+00,  6.1584e-01,  8.8871e-02, -4.2406e-01,\n",
       "          -9.7864e-01,  1.4694e-01,  5.7221e-01, -5.8380e-01,  2.3865e+00,\n",
       "           3.9268e-01,  4.0973e-01,  1.1463e+00,  1.5350e+00, -8.2074e-01,\n",
       "          -4.0709e-01,  1.6966e+00,  7.1687e-01,  1.8377e+00,  9.0361e-01,\n",
       "          -1.8764e-01, -9.1904e-01,  6.6169e-01, -8.7115e-02,  8.8748e-01,\n",
       "          -8.1187e-01, -9.1116e-01, -8.9069e-01,  5.0440e-02, -1.0968e+00,\n",
       "          -4.6413e-01, -1.9311e+00,  2.5017e-01,  5.3750e-01,  1.4273e+00,\n",
       "           9.9442e-01,  6.3607e-01, -1.2291e+00, -1.2514e+00,  5.7706e-01,\n",
       "          -1.2426e+00, -1.0695e+00,  9.3248e-01,  1.0893e+00, -6.9949e-02,\n",
       "           1.4333e+00,  3.2151e-01,  2.4957e-01, -3.7814e-01,  4.9070e-01,\n",
       "          -2.2358e-01, -6.5155e-02,  1.1322e-01, -1.7947e+00,  9.3702e-01,\n",
       "          -2.2629e+00, -4.4184e-01,  1.1947e-01, -2.0874e-01,  4.1096e-01,\n",
       "          -4.8757e-01, -1.4145e+00, -2.7676e-01, -5.9091e-01,  8.5322e-03,\n",
       "           7.4960e-01, -1.7456e-01, -3.5540e-01, -9.5795e-01, -1.4737e-01,\n",
       "          -2.1871e-01, -3.8056e-01,  4.9163e-01, -1.6124e+00, -3.1214e-01,\n",
       "           2.3860e+00, -2.3841e+00,  6.1423e-01]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        num_mem_kv = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        \n",
    "        self.norm = RMSNorm(dim)\n",
    "        \n",
    "        self.mem_kv = nn.Parameter(torch.randn(2, heads, dim_head, num_mem_kv))\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        \n",
    "        q, k, v = map(lambda t: rearrange('b (h c) x y -> b h c (x y)', h=self.heads))\n",
    "\n",
    "        mk, mv = map(lambda t: repeat(t, ))\n",
    "        \n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = heads * dim\n",
    "        \n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c85e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
